<!DOCTYPE html>
<html lang="ar" dir="rtl">
<head>
  <meta charset="UTF-8">
  <title>شرح LSTM</title>
  <style>
    body { font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; background: #f9f9f9; margin: 40px; }
    h1, h2 { color: #2c3e50; }
    .section { background: #fff; padding: 20px; margin-bottom: 20px; border-radius: 8px; box-shadow: 0 2px 8px #eee; }
    code, pre { background: #f4f4f4; padding: 8px; border-radius: 4px; display: block; direction: ltr; }
    ul { margin-right: 20px; }
  </style>
</head>
<body>
  <h1>شرح الشبكة العصبية LSTM</h1>

  <div class="section">
    <h2>1- الفكرة</h2>
    <p>
      LSTM (Long Short-Term Memory) هي نوع من الشبكات العصبية المتكررة (RNN) مصممة لمعالجة وحفظ المعلومات عبر تسلسل زمني طويل، وتتفوق في معالجة البيانات المتسلسلة مثل النصوص أو أسعار الأسهم لأنها تتغلب على مشكلة النسيان السريع في RNN التقليدية.
    </p>
  </div>

  <div class="section">
    <h2>2- الصيغة الرياضية</h2>
    <pre>
fₜ = σ(W_f · [hₜ₋₁, xₜ] + b_f)
iₜ = σ(W_i · [hₜ₋₁, xₜ] + b_i)
oₜ = σ(W_o · [hₜ₋₁, xₜ] + b_o)
ĉₜ = tanh(W_c · [hₜ₋₁, xₜ] + b_c)
cₜ = fₜ * cₜ₋₁ + iₜ * ĉₜ
hₜ = oₜ * tanh(cₜ)
    </pre>
  </div>

  <div class="section">
    <h2>3- شرح لمكونات الصيغة الرياضية</h2>
    <ul>
      <li><b>fₜ:</b> بوابة النسيان (Forget Gate) تحدد ما الذي سيتم نسيانه من الحالة السابقة.</li>
      <li><b>iₜ:</b> بوابة الإدخال (Input Gate) تحدد ما الذي سيتم إضافته للحالة الحالية.</li>
      <li><b>oₜ:</b> بوابة الإخراج (Output Gate) تحدد ما الذي سيتم إخراجه كحالة مخفية.</li>
      <li><b>ĉₜ:</b> الحالة المرشحة الجديدة (Candidate Cell State).</li>
      <li><b>cₜ:</b> الحالة الخلوية الحالية (Cell State).</li>
      <li><b>hₜ:</b> الحالة المخفية الحالية (Hidden State).</li>
      <li><b>σ:</b> دالة سيجمويد (Sigmoid Function).</li>
      <li><b>tanh:</b> دالة تانجينت التشعبية.</li>
      <li><b>W و b:</b> الأوزان والانحيازات التي يتم تعلمها أثناء التدريب.</li>
    </ul>
  </div>

  <div class="section">
    <h2>4- مثال مبسط لتطبيق الصيغة الرياضية</h2>
    <p>
      لنفترض أن لدينا تسلسل بيانات <b>xₜ</b> ونريد التنبؤ بالقيمة التالية. في كل خطوة زمنية، نقوم بحساب القيم السابقة باستخدام الأوزان المدربة، ثم نستخدم <b>hₜ</b> كمدخل للتنبؤ بالقيمة التالية.
    </p>
    <pre>
مثال رقمي:
xₜ = 0.5, hₜ₋₁ = 0.1, cₜ₋₁ = 0.2
نفترض W و b قيم بسيطة:
fₜ = σ(0.3*0.1 + 0.7*0.5 + 0.1) ≈ 0.62
iₜ = σ(0.4*0.1 + 0.6*0.5 + 0.2) ≈ 0.64
ĉₜ = tanh(0.5*0.1 + 0.5*0.5 + 0.1) ≈ 0.29
cₜ = 0.62*0.2 + 0.64*0.29 ≈ 0.12 + 0.19 ≈ 0.31
oₜ = σ(0.2*0.1 + 0.8*0.5 + 0.3) ≈ 0.68
hₜ = 0.68 * tanh(0.31) ≈ 0.68 * 0.30 ≈ 0.20
    </pre>
    <p>
      <b>hₜ</b> هنا هو التنبؤ أو الحالة المخفية التي يمكن استخدامها للتنبؤ بالقيمة التالية.
    </p>
  </div>

  <div class="section">
    <h2>5- المميزات</h2>
    <ul>
      <li>قادرة على حفظ المعلومات لفترات زمنية طويلة.</li>
      <li>تتغلب على مشكلة تلاشي أو انفجار القيم في RNN التقليدية.</li>
      <li>مناسبة جدًا لتسلسل البيانات مثل النصوص أو أسعار الأسهم.</li>
    </ul>
  </div>

  <div class="section">
    <h2>6- العيوب</h2>
    <ul>
      <li>تحتاج وقتًا أطول في التدريب مقارنة بـ RNN التقليدية.</li>
      <li>تحتوي على عدد كبير من المعاملات مما يزيد من التعقيد الحسابي.</li>
      <li>قد تتطلب بيانات كثيرة للحصول على نتائج جيدة.</li>
    </ul>
  </div>

  <div class="section">
    <h2>7- الكود المستخدم</h2>
    <pre>
import numpy as np
from keras.models import Sequential
from keras.layers import LSTM, Dense

# تحضير البيانات (مثال مبسط)
X = np.random.rand(100, 10, 1)  # 100 عينة، كل عينة طولها 10، بعد واحد
y = np.random.rand(100, 1)

# بناء النموذج
model = Sequential()
model.add(LSTM(50, input_shape=(10, 1)))
model.add(Dense(1))

model.compile(optimizer='adam', loss='mse')
model.fit(X, y, epochs=10, batch_size=16)
    </pre>
  </div>
</body>
</html>
